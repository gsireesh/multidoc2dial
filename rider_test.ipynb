{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from transformers import RagTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dialdoc.models.rag.distributed_pytorch_retriever import RagPyTorchDistributedRetriever\n",
    "from dialdoc.models.rag.modeling_rag_dialdoc import DialDocRagTokenForGeneration\n",
    "from dialdoc.models.rag.configuration_rag_dialdoc import DialDocRagConfig\n",
    "from dialdoc.models.rag import rider\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"checkpoints/rag-dpr-all-structure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DialDocRagConfig.from_pretrained(MODEL_PATH)\n",
    "config.bm25 = None\n",
    "config.index_name = \"dialdoc\"\n",
    "config.passages_path = \"data/mdd_kb/knowledge_dataset-dpr-all-structure/my_knowledge_dataset\"\n",
    "config.index_path = \"data/mdd_kb/knowledge_dataset-dpr-all-structure/my_knowledge_dataset_index.faiss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = Namespace(logger=True, checkpoint_callback=True, default_root_dir=None, gradient_clip_val=0.1, process_position=0, num_nodes=1, num_processes=1, gpus=1, auto_select_gpus=False, log_gpu_memory=None, progress_bar_refresh_rate=1, overfit_batches=0.0, track_grad_norm=-1, check_val_every_n_epoch=1, fast_dev_run=False, accumulate_grad_batches=1, max_epochs=2, min_epochs=1, max_steps=None, min_steps=None, limit_train_batches=1.0, limit_val_batches=1.0, limit_test_batches=1.0, val_check_interval=1.0, flush_logs_every_n_steps=100, log_every_n_steps=50, accelerator=None, sync_batchnorm=False, precision=32, weights_summary='top', weights_save_path=None, num_sanity_val_steps=2, truncated_bptt_steps=None, resume_from_checkpoint=None, profiler=None, benchmark=False, deterministic=False, reload_dataloaders_every_epoch=False, auto_lr_find=False, replace_sampler_ddp=True, terminate_on_nan=False, auto_scale_batch_size=False, prepare_data_per_node=True, plugins=None, amp_backend='native', amp_level='O2', distributed_backend=None, automatic_optimization=None, move_metrics_to_cpu=False, enable_pl_optimizer=None, model_name_or_path='/usr0/home/sgururaj/src/11-797-multidoc2dial/multidoc2dial/checkpoints/rag-dpr-all-structure', config_name='', tokenizer_name=None, cache_dir='', encoder_layerdrop=None, decoder_layerdrop=None, dropout=0.1, attention_dropout=0.1, learning_rate=3e-05, lr_scheduler='polynomial', weight_decay=0.001, adam_epsilon=1e-08, warmup_steps=500, num_workers=4, train_batch_size=8, eval_batch_size=2, adafactor=False, output_dir='/usr0/home/sgururaj/src/11-797-multidoc2dial/multidoc2dial/checkpoints/mdd-generation-dpr-all-structure-original', fp16=True, fp16_opt_level='O2', do_train=True, do_predict=False, seed=15942, data_dir='../data/mdd_all/dd-generation-structure', scoring_func='original', segmentation='structure', bm25=None, max_combined_length=300, max_source_length=128, max_target_length=50, val_max_target_length=50, test_max_target_length=50, logger_name='default', n_train=-1, n_val=-1, n_test=-1, label_smoothing=0.1, prefix=None, early_stopping_patience=-1, distributed_port=-1, model_type='rag_token_dialdoc', n_docs=5, index_name='dialdoc', passages_path='../data/mdd_kb/knowledge_dataset-dpr-all-structure/my_knowledge_dataset', index_path='../data/mdd_kb/knowledge_dataset-dpr-all-structure/my_knowledge_dataset_index.faiss', mapping_file=None, distributed_retriever='pytorch', use_dummy_dataset=False, do_marginalize=True, ray_address='auto', num_retrieval_workers=1, profile=True, actor_handles=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RagTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RagPyTorchDistributedRetriever.from_pretrained(MODEL_PATH, config=config)\n",
    "retriever.init_retrieval(9433)\n",
    "model = DialDocRagTokenForGeneration.from_pretrained(MODEL_PATH, config=config, retriever=retriever, bm25=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    i = tokenizer(\"This is a test sentence\")[\"input_ids\"]\n",
    "    print(tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Hello, I forgot o update my address, can you help me with that?[SEP]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer(question, return_tensors=\"pt\")\n",
    "input_ids = torch.vstack((tokenized[\"input_ids\"], tokenized[\"input_ids\"]))\n",
    "attn_mask = torch.vstack((tokenized[\"attention_mask\"], tokenized[\"attention_mask\"]))\n",
    "token_type_ids = torch.vstack((tokenized[\"token_type_ids\"], tokenized[\"token_type_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpr_out = model.question_encoder(input_ids, attn_mask, output_hidden_states=True, return_dict=True)\n",
    "combined_out = dpr_out.pooler_output\n",
    "sequence_output = dpr_out.hidden_states[-1]\n",
    "attn_mask = model.get_attn_mask(input_ids)\n",
    "## Split sequence output, and pool each sequence\n",
    "seq_out_0 = []  # last turn, if query; doc structure if passage\n",
    "seq_out_1 = []  # dial history, if query; passage text if passage\n",
    "dialog_lengths = []\n",
    "for i in range(sequence_output.shape[0]):\n",
    "    seq_out_masked = sequence_output[i, attn_mask[i], :]\n",
    "    segment_masked = token_type_ids[i, attn_mask[i]]\n",
    "    seq_out_masked_0 = seq_out_masked[segment_masked == 0, :]\n",
    "    seq_out_masked_1 = seq_out_masked[segment_masked == 1, :]\n",
    "    dialog_lengths.append((len(seq_out_masked_0), len(seq_out_masked_1)))\n",
    "    ### perform pooling\n",
    "    seq_out_0.append(model.mean_pool(seq_out_masked_0))\n",
    "    seq_out_1.append(model.mean_pool(seq_out_masked_1))\n",
    "\n",
    "pooled_output_0 = torch.cat([seq.view(1, -1) for seq in seq_out_0], dim=0)\n",
    "pooled_output_1 = torch.cat([seq.view(1, -1) for seq in seq_out_1], dim=0)\n",
    "\n",
    "current_out = pooled_output_0\n",
    "\n",
    "retrieved = model.retriever(\n",
    "    input_ids,\n",
    "    combined_out.cpu().detach().to(torch.float32).numpy(),\n",
    "    combined_out.cpu().detach().to(torch.float32).numpy(),  ## sending dummy\n",
    "    combined_out.cpu().detach().to(torch.float32).numpy(),  ## sending dummy\n",
    "    prefix=model.generator.config.prefix,\n",
    "    n_docs=n_docs,\n",
    "    dialog_lengths=dialog_lengths,\n",
    "    domain=\"dmv\",\n",
    "    return_tensors=\"pt\",\n",
    "    bm25=model.bm25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dict_list = retriever.index.get_doc_dicts(retrieved.doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = model(**tokenized, do_marginalize=True, output_retrieved=True)\n",
    "tokenizer.decode(torch.squeeze(generated.logits.argmax(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = model.generate(\n",
    "    context_input_ids=retrieved[\"context_input_ids\"],\n",
    "    context_attention_mask=retrieved[\"context_attention_mask\"], \n",
    "    doc_scores=retrieved.doc_scores, \n",
    "    num_beams=4, \n",
    "    num_return_sequences=4\n",
    ")\n",
    "\n",
    "generated_strings = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "print(\"\\n\".join(generated_strings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_question(question, tokenizer, model, n_docs=5):\n",
    "    tokenized = tokenizer(question, return_tensors=\"pt\")\n",
    "    \n",
    "    dpr_out = model.question_encoder(tokenized.input_ids, tokenized.attention_mask, output_hidden_states=True, return_dict=True)\n",
    "    combined_out = dpr_out.pooler_output\n",
    "    sequence_output = dpr_out.hidden_states[-1]\n",
    "    attn_mask = model.get_attn_mask(tokenized.input_ids)\n",
    "    ## Split sequence output,| and pool each sequence\n",
    "    seq_out_0 = []  # last turn, if query; doc structure if passage\n",
    "    seq_out_1 = []  # dial history, if query; passage text if passage\n",
    "    dialog_lengths = []\n",
    "    for i in range(sequence_output.shape[0]):\n",
    "        seq_out_masked = sequence_output[i, attn_mask[i], :]\n",
    "        segment_masked = tokenized.token_type_ids[i, attn_mask[i]]\n",
    "        seq_out_masked_0 = seq_out_masked[segment_masked == 0, :]\n",
    "        seq_out_masked_1 = seq_out_masked[segment_masked == 1, :]\n",
    "        dialog_lengths.append((len(seq_out_masked_0), len(seq_out_masked_1)))\n",
    "        ### perform pooling\n",
    "        seq_out_0.append(model.mean_pool(seq_out_masked_0))\n",
    "        seq_out_1.append(model.mean_pool(seq_out_masked_1))\n",
    "\n",
    "    pooled_output_0 = torch.cat([seq.view(1, -1) for seq in seq_out_0], dim=0)\n",
    "    pooled_output_1 = torch.cat([seq.view(1, -1) for seq in seq_out_1], dim=0)\n",
    "\n",
    "    current_out = pooled_output_0\n",
    "\n",
    "    retrieved = model.retriever(\n",
    "        tokenized.input_ids,\n",
    "        combined_out.cpu().detach().to(torch.float32).numpy(),\n",
    "        combined_out.cpu().detach().to(torch.float32).numpy(),  ## sending dummy\n",
    "        combined_out.cpu().detach().to(torch.float32).numpy(),  ## sending dummy\n",
    "        prefix=model.generator.config.prefix,\n",
    "        n_docs=n_docs,\n",
    "        dialog_lengths=dialog_lengths,\n",
    "        domain=\"dmv\",\n",
    "        return_tensors=\"pt\",\n",
    "        bm25=model.bm25,\n",
    "    )\n",
    "\n",
    "\n",
    "    generated = model.generate(\n",
    "    context_input_ids=retrieved[\"context_input_ids\"],\n",
    "    context_attention_mask=retrieved[\"context_attention_mask\"], \n",
    "    doc_scores=retrieved.doc_scores, \n",
    "    num_beams=4, \n",
    "    num_return_sequences=4,\n",
    "    n_docs=n_docs\n",
    "    )\n",
    "\n",
    "    return retrieved, tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr0/home/sgururaj/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/transformers/generation_utils.py:1731: UserWarning: `max_length` is deprecated in this function, use `stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length))` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'context_input_ids': tensor([[    0, 42891,     6,  ...,     1,     1,     1],\n",
       "         [    0, 42891,     6,  ...,     1,     1,     1],\n",
       "         [    0, 42891,     6,  ...,     1,     1,     1],\n",
       "         [    0, 42891,     6,  ...,     1,     1,     1]]), 'context_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]), 'retrieved_doc_embeds': tensor([[[-0.0722,  0.0278,  0.4568,  ...,  0.1845, -0.3385,  0.1325],\n",
       "          [-0.0600,  0.4052,  0.5585,  ..., -0.3723, -0.6438,  0.0931],\n",
       "          [ 0.3052,  0.1145,  0.6771,  ..., -0.4292, -0.4348,  0.0487],\n",
       "          [ 0.1788, -0.1359,  0.5161,  ...,  0.0727, -0.2167, -0.2042]]]), 'doc_ids': tensor([[1804, 2419, 2607, 1989]]), 'doc_scores': tensor([[80.1930, 77.2222, 73.7971, 72.1576]])},\n",
       " ['hello.hello, i forgot o update my address, can you help me can you help',\n",
       "  'hello.hello, i forgot o update my address, i forgot o update my address,',\n",
       "  'hello.hello, i forgot o update my address, can you help me help me can',\n",
       "  'hello.hello, i forgot o update my address, can you help me help me o'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_from_question(question, tokenizer, model, n_docs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/mdd_all/dd-generation-structure/train.source\") as f:\n",
    "    question_lines = f.readlines()\n",
    "\n",
    "with open(\"data/mdd_all/dd-generation-structure/train.pids\") as f:\n",
    "    pid_lines = [int(line.strip()) for line in f.readlines()]\n",
    "\n",
    "with open(\"data/mdd_all/dd-generation-structure/train.target\") as f:\n",
    "    answer_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/usr0/home/sgururaj/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/transformers/generation_utils.py:1731: UserWarning: `max_length` is deprecated in this function, use `stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length))` instead.\n",
      "  warnings.warn(\n",
      "11it [01:43,  9.40s/it]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "q2pred = {}\n",
    "\n",
    "i = 0\n",
    "\n",
    "for question, correct_pid, answer in tqdm(zip(question_lines, pid_lines, answer_lines)):\n",
    "    retrieved, preds = generate_from_question(question, tokenizer, model, n_docs=10)\n",
    "    doc_ids = retrieved.doc_ids[0]\n",
    "    d = {}\n",
    "    d[\"question\"] = question\n",
    "    d[\"answers\"] = [answer]\n",
    "    d[\"ctxs\"] = []\n",
    "    q2pred[question] = preds\n",
    "\n",
    "\n",
    "    docs_dict = retriever.index.get_doc_dicts(doc_ids)  \n",
    "\n",
    "    for (doc_id, doc) in zip(doc_ids, docs_dict):\n",
    "         d[\"ctxs\"].append({\n",
    "             \"id\": doc_id.item(),\n",
    "             \"title\": doc[\"title\"],\n",
    "             \"text\": doc[\"text\"],\n",
    "             \"has_answer\": doc_id.item() == correct_pid\n",
    "         })\n",
    "    \n",
    "    data.append(d)\n",
    "    if i > 10:\n",
    "        break\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [05:33<00:00, 27.83s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t old   rerank\n",
      "top-1 acc:\t 1.000 1.000\n",
      "top-5 acc:\t 1.000 1.000\n",
      "top-10 acc:\t 1.000 1.000\n",
      "top-20 acc:\t 1.000 1.000\n",
      "top-100 acc:\t 1.000 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rider.rider_rerank_measure(data, q2pred, n_ctxs=5, n_pred=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d8a4f1363901701428854a984839ebd364ac476baeb84c240afb7849b9be1bc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('multidoc2dial')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
